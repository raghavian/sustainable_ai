# Chapter 1: Sustainability and Artificial Intelligence

- Intergovernmental Panel on Climate Change (IPCC). *Climate Change 2023: Synthesis Report. Contribution of Working Groups I, II, and III to the Sixth Assessment Report of the Intergovernmental Panel on Climate Change.* Geneva: IPCC, 2023. https://doi.org/10.59327/IPCC/AR6-9789291691647

- World Commission on Environment and Development. *Our Common Future.* Oxford: Oxford University Press, 1987.

- Jantzen, Jan, Michael Kristensen, and Toke Haunstrup Christensen. “Sociotechnical Transition to Smart Energy: The Case of Samsø 1997–2030.” *Energy* 162 (August 3, 2018): 20–34. https://doi.org/10.1016/j.energy.2018.07.174

- United Nations Framework Convention on Climate Change (UNFCCC). “Samsø: An Island Community Pointing to the Future.” 2023. https://unfccc.int/climate-action/un-global-climate-action-awards/climate-leaders/samso

- McCarthy, John, Marvin Minsky, Nathaniel Rochester, and Claude E. Shannon. “Dartmouth Summer Research Project on Artificial Intelligence.” 1956. Related proposal DOI: https://doi.org/10.1609/aimag.v27i4.1904

- Schmidhuber, Juergen. “Annotated History of Modern AI and Deep Learning.” *arXiv.org*, December 21, 2022. https://doi.org/10.48550/arXiv.2212.11279

- Lynch, Shana. “Andrew Ng: Why AI Is the New Electricity.” Stanford Graduate School of Business, March 11, 2017. https://www.gsb.stanford.edu/insights/andrew-ng-why-ai-new-electricity

- Pasquinelli, Matteo. *The Eye of the Master: A Social History of Artificial Intelligence.* London: Verso Books, 2023.

- Murgia, Madhumita, and Nathalie Thomas. “DeepMind and National Grid in AI Talks to Balance Energy Supply.” *Financial Times*, March 11, 2017. https://www.ft.com/content/27c8aea0-06a9-11e7-97d1-5e720a26771b

- Meister, Miriam. “AI Predicts Flooding.” Technical University of Denmark, March 7, 2024. https://www.dtu.dk/english/newsarchive/2024/03/ai-predicts-flooding

- Nearing, Grey, Deborah Cohen, Vusumuzi Dube, Martin Gauch, Oren Gilon, Shaun Harrigan, Avinatan Hassidim, et al. “Global Prediction of Extreme Floods in Ungauged Watersheds.” *Nature* 627 (March 20, 2024): 559–63. https://doi.org/10.1038/s41586-024-07145-1

- Ziemba, Ewa Wanda, Cong Doanh Duong, Joanna Ejdys, Maria Alejandra Gonzalez-Perez, Ruta Kazlauskaitė, Pawel Korzynski, Grzegorz Mazurek, Joanna Paliszkiewicz, Jelena Stankevičienė, and Krzysztof Wach. “Leveraging Artificial Intelligence to Meet the Sustainable Development Goals.” *Journal of Economics and Management* 46 (December 16, 2024): 508–83. https://doi.org/10.22367/jem.2024.46.19

- Yeh, Christopher, Anthony Perez, Anne Driscoll, George Azzari, Zhongyi Tang, David Lobell, Stefano Ermon, and Marshall Burke. “Using Publicly Available Satellite Imagery and Deep Learning to Understand Economic Well-Being in Africa.” *Nature Communications* 11 (May 22, 2020). https://doi.org/10.1038/s41467-020-16185-w

- McKinney, Scott Mayer, Marcin Sieniek, Varun Godbole, Jonathan Godwin, Natasha Antropova, Hutan Ashrafian, Trevor Back, et al. “International Evaluation of an AI System for Breast Cancer Screening.” *Nature* 577, no. 7788 (January 1, 2020): 89–94. https://doi.org/10.1038/s41586-019-1799-6

- Oladapo, Bankole I., Mattew A. Olawumi, and Francis T. Omigbodun. “Machine Learning for Optimising Renewable Energy and Grid Efficiency.” *Atmosphere* 15, no. 10 (October 19, 2024): 1250. https://doi.org/10.3390/atmos15101250

- Paolo, Fernando S., David Kroodsma, Jennifer Raynor, Tim Hochberg, Pete Davis, Jesse Cleary, Luca Marsaglia, Sara Orofino, Christian Thomas, and Patrick Halpin. “Satellite Mapping Reveals Extensive Industrial Activity at Sea.” *Nature* 625, no. 7993 (January 3, 2024): 85–91. https://doi.org/10.1038/s41586-023-06825-8

- Morozov, Evgeny. *To Save Everything, Click Here: Technology, Solutionism and the Urge to Fix Problems That Don’t Exist.* Review in *Information Polity* 18, no. 3 (July 1, 2013): 275–76. (No DOI available.)

- Brevini, Benedetta. “Myths, Techno-Solutionism and Artificial Intelligence: Reclaiming AI Materiality and Its Massive Environmental Costs.” In *Handbook of Critical Studies of Artificial Intelligence*, edited by Simon Lindgren, 869–77. Cheltenham, UK: Edward Elgar Publishing, 2023. https://doi.org/10.4337/9781803928562.00086

- Kaplan, Jared, Sam McCandlish, Tom Henighan, Tom B. Brown, Benjamin Chess, Rewon Child, Scott Gray, et al. “Scaling Laws for Neural Language Models.” *arXiv.org*, January 23, 2020. https://doi.org/10.48550/arXiv.2001.08361

- Bakhtiarifard, Pedram, Pınar Tözün, Christian Igel, and Raghavendra Selvan. “Climate and Resource Awareness Is Imperative to Achieving Sustainable AI (and Preventing a Global AI Arms Race).” *arXiv*, February 27, 2025. https://doi.org/10.48550/arXiv.2502.20016

- International Energy Agency (IEA). *Energy and AI.* Paris: IEA, 2025. https://www.iea.org/reports/energy-and-ai

- International Energy Agency (IEA). *Electricity 2025.* Paris: IEA, 2025. https://www.iea.org/reports/electricity-2025

- Bickerstaff, Karen, Gordon Walker, and Harriet Bulkeley, eds. *Energy Justice in a Changing Climate: Social Equity and Low-Carbon Energy.* London: Zed Books, 2013.

- Li, Pengfei, Jianyi Yang, Mohammad A. Islam, and Shaolei Ren. “Making AI Less ‘Thirsty’: Uncovering and Addressing the Secret Water Footprint of AI Models.” *Communications of the ACM* 68, no. 7 (2025): 54–61. https://doi.org/10.1145/3724499

- Ministry of Jal Shakti (India). *Annual Report 2024–25.* Government of India, 2025. https://www.jalshakti-dowr.gov.in/static/uploads/2024/05/fc00cd887135cf39b2005ccf1539e0e5.pdf

- Tan, Eli, and Dustin Chambers. “Their Water Taps Ran Dry When Meta Built Next Door.” *New York Times*, July 14, 2025. https://www.nytimes.com/2025/07/14/technology/meta-data-center-water.html

- Kokkegård, Hanne. “Utilize Waste Heat From Data Centers in District Heating.” Technical University of Denmark, November 1, 2022. https://www.dtu.dk/english/news/all-news/nyhed?id=b2e4c8f0-0c62-437d-afbf-d2a17c376ffa

- Temple, James. “The Data Center Boom in the Desert.” *MIT Technology Review*, May 20, 2025. https://www.technologyreview.com/2025/05/20/1116287/ai-data-centers-nevada-water-reno-computing-environmental-impact/

- Luccioni, Alexandra Sasha, Sylvain Viguier, and Anne-Laure Ligozat. “Estimating the Carbon Footprint of BLOOM, a 176B Parameter Language Model.” *Journal of Machine Learning Research* 24, no. 1 (2023): 11990–12004. https://www.jmlr.org/papers/v24/23-0069.html

- Giglio, Elena. “Extractivism and Its Socio-Environmental Impact in South America. Overview of the ‘Lithium Triangle.’” *América Crítica* 5, no. 1 (2021): 47–53. https://doi.org/10.13125/americacritica/4926

- Schwartz, Roy, Jesse Dodge, Noah A. Smith, and Oren Etzioni. “Green AI.” *Communications of the ACM* 63, no. 12 (2020): 54–63. https://doi.org/10.1145/3381831

- Khazzoom, J. Daniel. “Economic Implications of Mandated Efficiency in Standards for Household Appliances.” *The Energy Journal* 1, no. 4 (1980): 21–40. https://doi.org/10.5547/issn0195-6574-ej-vol1-no4-2

- Wright, Dustin, Christian Igel, Gabrielle Samuel, and Raghavendra Selvan. “Efficiency Is Not Enough: A Critical Perspective of Environmentally Sustainable AI.” *Communications of the ACM* 68, no. 7 (2025): 62–69. https://doi.org/10.1145/3724500

# Chapter 2: Under the Hood of Generative AI

- Brown, Tom B., Benjamin Mann, Nick Ryder, Melanie Subbiah, Jared D. Kaplan, Prafulla Dhariwal, Arvind Neelakantan, et al. “Language Models Are Few-Shot Learners.” *arXiv*, May 28, 2020. https://doi.org/10.48550/arXiv.2005.14165

- Rombach, Robin, Andreas Blattmann, Dominik Lorenz, Patrick Esser, and Björn Ommer. “High-Resolution Image Synthesis with Latent Diffusion Models.” *arXiv*, December 20, 2021. https://doi.org/10.48550/arXiv.2112.10752

- Agostinelli, Andrea, Timo I. Denk, Zalán Borsos, Jesse Engel, Mauro Verzetti, Antoine Caillon, Qingqing Huang, et al. “MusicLM: Generating Music from Text.” *arXiv*, January 26, 2023. https://doi.org/10.48550/arXiv.2301.11325

- Bender, Emily M. “Resisting Dehumanization in the Age of ‘AI.’” *Current Directions in Psychological Science* 33, no. 2 (2024): 114–120. https://doi.org/10.1177/09637214231217286

- Bakhtiarifard, Pedram, Pınar Tözün, Christian Igel, and Raghavendra Selvan. “Climate and Resource Awareness Is Imperative to Achieving Sustainable AI (and Preventing a Global AI Arms Race).” *arXiv*, February 27, 2025. https://doi.org/10.48550/arXiv.2502.20016

- Bengio, Yoshua, Aaron Courville, and Pascal Vincent. “Representation Learning: A Review and New Perspectives.” *arXiv*, June 24, 2012. https://doi.org/10.48550/arXiv.1206.5538

- Gonzalez, Rafael C., and Richard E. Woods. *Digital Image Processing*. 2nd ed. Reading, MA: Addison-Wesley, 1987.

- Kramer, Mark A. “Nonlinear Principal Component Analysis Using Autoassociative Neural Networks.” *AIChE Journal* 37, no. 2 (1991): 233–243. https://doi.org/10.1002/aic.690370209

- Goodfellow, Ian, Yoshua Bengio, and Aaron Courville. “Autoencoders.” In *Deep Learning*. Cambridge, MA: MIT Press, 2016. https://www.deeplearningbook.org/contents/autoencoders.html

- Kingma, Diederik P., and Max Welling. “Auto-Encoding Variational Bayes.” *arXiv*, December 20, 2013. https://doi.org/10.48550/arXiv.1312.6114

- Radford, Alec, Jong Wook Kim, Chris Hallacy, Aditya Ramesh, Gabriel Goh, Sandhini Agarwal, Girish Sastry, et al. “Learning Transferable Visual Models from Natural Language Supervision.” *Proceedings of the 38th International Conference on Machine Learning (ICML 2021)*, 8748–8763. Also *arXiv*, February 26, 2021. https://doi.org/10.48550/arXiv.2103.00020

- Rosenblatt, Frank. “The Perceptron: A Probabilistic Model for Information Storage and Organization in the Brain.” *Psychological Review* 65, no. 6 (1958): 386–408. https://doi.org/10.1037/h0042519

- Rumelhart, David E., Geoffrey E. Hinton, and Ronald J. Williams. “Learning Representations by Back-Propagating Errors.” *Nature* 323 (October 9, 1986): 533–536. https://doi.org/10.1038/323533a0

- LeCun, Yann, Léon Bottou, Yoshua Bengio, and Patrick Haffner. “Gradient-Based Learning Applied to Document Recognition.” *Proceedings of the IEEE* 86, no. 11 (1998): 2278–2324. https://doi.org/10.1109/5.726791

- Elman, Jeffrey L. “Finding Structure in Time.” *Cognitive Science* 14, no. 2 (1990): 179–211. https://doi.org/10.1207/s15516709cog1402_1

- Hochreiter, Sepp, and Jürgen Schmidhuber. “Long Short-Term Memory.” *Neural Computation* 9, no. 8 (1997): 1735–1780. https://doi.org/10.1162/neco.1997.9.8.1735

- Kipf, Thomas N., and Max Welling. “Semi-Supervised Classification with Graph Convolutional Networks.” *arXiv*, September 9, 2016. https://doi.org/10.48550/arXiv.1609.02907

- Vaswani, Ashish, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones, Aidan N. Gomez, Łukasz Kaiser, and Illia Polosukhin. “Attention Is All You Need.” *arXiv*, June 12, 2017. https://doi.org/10.48550/arXiv.1706.03762

- Duman Keles, Feyza, Pruthuvi Mahesakya Wijewardena, and Chinmay Hegde. “On the Computational Complexity of Self-Attention.” *arXiv*, September 11, 2022. https://arxiv.org/abs/2209.04881

- Grefenstette, Gregory. “Tokenization.” In *Syntactic Wordclass Tagging*, edited by H. van Halteren, 117–133. Dordrecht: Springer, 1999. https://doi.org/10.1007/978-94-015-9273-4_9

- Ruder, Sebastian. “An Overview of Gradient Descent Optimization Algorithms.” *arXiv*, September 15, 2016. https://doi.org/10.48550/arXiv.1609.04747

- Baydin, Atilim Gunes, Barak A. Pearlmutter, Alexey Andreyevich Radul, and Jeffrey Mark Siskind. “Automatic Differentiation in Machine Learning: A Survey.” *Journal of Machine Learning Research* 18 (2018): 1–43. Preprint *arXiv*, February 20, 2015. https://doi.org/10.48550/arXiv.1502.05767

- Kingma, Diederik P., and Jimmy Ba. “Adam: A Method for Stochastic Optimization.” *arXiv*, December 22, 2014. https://doi.org/10.48550/arXiv.1412.6980

- Lex Clips. “Transformers: The Best Idea in AI | Andrej Karpathy and Lex Fridman.” YouTube video, November 1, 2022. https://youtu.be/9uw3F6rndnA?t=393

- Sevilla, Jaime, Lennart Heim, Anson Ho, Tamay Besiroglu, Marius Hobbhahn, and Pablo Villalobos. “Compute Trends Across Three Eras of Machine Learning.” *2022 International Joint Conference on Neural Networks (IJCNN)*, July 18, 2022: 1–8. https://doi.org/10.1109/IJCNN55064.2022.9891914. See also the preprint: *arXiv*, February 11, 2022. https://arxiv.org/abs/2202.05924

# Chapter 3: Quantifying the Efficiency of Deep Learning

- Sevilla, Jaime, Lennart Heim, Anson Ho, Tamay Besiroglu, Marius Hobbhahn, and Pablo Villalobos. “Compute Trends Across Three Eras of Machine Learning.” *Proceedings of the 2022 International Joint Conference on Neural Networks (IJCNN)*, July 18, 2022, 1–8. https://doi.org/10.1109/IJCNN55064.2022.9891914

- Wright, Dustin, Christian Igel, Gabrielle Samuel, and Raghavendra Selvan. “Efficiency Is Not Enough: A Critical Perspective of Environmentally Sustainable AI.” *Communications of the ACM* 68, no. 7 (2025): 62–69. https://doi.org/10.1145/3724500

- Menabrea, L. F. “Sketch of the Analytical Engine Invented by Charles Babbage, Esq.” *Bibliothèque Universelle de Genève* (October 1842). English translation with notes by Ada Lovelace, 1843. http://psychclassics.yorku.ca/Lovelace/menabrea.htm

- Intergovernmental Panel on Climate Change (IPCC). *Climate Change 2014: Mitigation of Climate Change. Contribution of Working Group III to the Fifth Assessment Report of the Intergovernmental Panel on Climate Change.* Cambridge: Cambridge University Press, 2014. https://doi.org/10.1017/CBO9781107415416

- Freitag, Charlotte, Mike Berners-Lee, Kelly Widdicks, Bran Knowles, Gordon S. Blair, and Adrian Friday. “The Real Climate and Transformative Impact of ICT: A Critique of Estimates, Trends, and Regulations.” *Patterns* 2, no. 9 (2021): 100340. https://doi.org/10.1016/j.patter.2021.100340

- Purvis, Ben, Yong Mao, and Darren Robinson. “Three Pillars of Sustainability: In Search of Conceptual Origins.” *Sustainability Science* 14, no. 3 (2019): 681–695. https://doi.org/10.1007/s11625-018-0627-5

- Strassen, Volker. “Gaussian Elimination Is Not Optimal.” *Numerische Mathematik* 13, no. 4 (1969): 354–356. https://doi.org/10.1007/BF02165411

- Fawzi, Alhussein, Matej Balog, Aja Huang, Thomas Hubert, Bernardino Romera-Paredes, Mohammadamin Barekatain, Alexander Novikov, et al. “Discovering Faster Matrix Multiplication Algorithms with Reinforcement Learning.” *Nature* 610, no. 7930 (2022): 47–53. https://doi.org/10.1038/s41586-022-05172-4

- Ahmed, Nur, and Muntasir Wahed. “The De-democratization of AI: Deep Learning and the Compute Divide in Artificial Intelligence Research.” *arXiv* (October 22, 2020). https://doi.org/10.48550/arXiv.2010.15581

- Dodge, Jesse, Taylor Prewitt, Remi Tachet des Combes, Erika Odmark, Roy Schwartz, Emma Strubell, Alexandra Sasha Luccioni, Noah A. Smith, Nicole DeCario, and Will Buchanan. “Measuring the Carbon Intensity of AI in Cloud Instances.” In *Proceedings of the 2022 ACM Conference on Fairness, Accountability, and Transparency (FAccT ’22)*, June 20–24, 2022, 1877–1894. https://doi.org/10.1145/3531146.3533234

- Hwang, Tim. “Computational Power and the Social Impact of Artificial Intelligence.” *arXiv* (March 23, 2018). https://doi.org/10.48550/arXiv.1803.08971

- Strubell, Emma, Ananya Ganesh, and Andrew McCallum. “Energy and Policy Considerations for Deep Learning in NLP.” *arXiv* (June 5, 2019). https://doi.org/10.48550/arXiv.1906.02243

- Polimeni, John M., Kozo Mayumi, Mario Giampietro, and Blake Alcott. *The Myth of Resource Efficiency: The Jevons Paradox.* London: Earthscan, 2008.

- Radovanovic, Ana, Ross Koningstein, Ian Schneider, Bokan Chen, Alexandre Duarte, Binz Roy, Diyue Xiao, et al. “Carbon-Aware Computing for Datacenters.” *arXiv* (June 11, 2021). https://doi.org/10.48550/arXiv.2106.11750

- Anthony, Lasse F. Wolff, Benjamin Kanding, and Raghavendra Selvan. “Carbontracker: Tracking and Predicting the Carbon Footprint of Training Deep Learning Models.” *arXiv* (July 6, 2020). https://doi.org/10.48550/arXiv.2007.03051

- Bouza, Lucía, Aurélie Bugeau, and Loïc Lannelongue. “How to Estimate Carbon Footprint When Training Deep Learning Models? A Guide and Review.” *Environmental Research Communications* 5, no. 11 (2023): 115014. https://doi.org/10.1088/2515-7620/acf81b

- Wolpert, David H., and William G. Macready. “No Free Lunch Theorems for Optimization.” *IEEE Transactions on Evolutionary Computation* 1, no. 1 (1997): 67–82. https://doi.org/10.1109/4235.585893

- Patterson, David, Joseph Gonzalez, Urs Hölzle, Quoc Le, Chen Liang, Lluis-Miquel Munguia, Daniel Rothchild, David So, Maud Texier, and Jeff Dean. “The Carbon Footprint of Machine Learning Training Will Plateau, Then Shrink.” *arXiv* (April 11, 2022). https://doi.org/10.48550/arXiv.2204.05149


# Chapter 4: Data Parsimony

- Grattafiori, Aaron, Abhimanyu Dubey, Abhinav Jauhri, Abhinav Pandey, Abhishek Kadian, Ahmad Al-Dahle, Aiesha Letman, et al. “The Llama 3 Herd of Models.” *arXiv*, 2024. https://doi.org/10.48550/arXiv.2407.21783

- International Energy Agency. “Data Centres and Data Transmission Networks.” IEA, 2023. https://www.iea.org/analyses/data-centres-and-data-transmission-networks

- Freitag, Charlotte, Mike Berners-Lee, Kelly Widdicks, Bran Knowles, Gordon S. Blair, and Adrian Friday. “The Climate Impact of ICT: A Review of Estimates, Trends and Regulations.” *arXiv*, 2021. https://doi.org/10.48550/arXiv.2102.02622

- Seagate Technology LLC. *Enterprise Capacity 3.5 HDD: v5.1 Product Manual.* September 2017. https://www.seagate.com/files/www-content/product-content/enterprise-hdd-fam/enterprise-capacity-3-5-hdd/enterprise-capacity-3-5-hdd/en-us/docs/100805922c.pdf

- Smith, Shaden, Mostofa Patwary, Brandon Norick, Patrick LeGresley, Samyam Rajbhandari, Jared Casper, Zhun Liu, et al. “Using DeepSpeed and Megatron to Train Megatron-Turing NLG 530B, a Large-Scale Generative Language Model.” *arXiv*, 2022. https://doi.org/10.48550/arXiv.2201.11990

- Hoffmann, Jordan, Sebastian Borgeaud, Arthur Mensch, Elena Buchatskaya, Trevor Cai, Eliza Rutherford, Diego de Las Casas, et al. “Training Compute-Optimal Large Language Models.” *arXiv*, 2022. https://doi.org/10.48550/arXiv.2203.15556

- Villalobos, Pablo, and Anson Ho. *Trends in Training Dataset Sizes.* Epoch AI, 2022. https://epoch.ai/blog/trends-in-training-dataset-sizes

- Anthony, Lasse F. Wolff, Benjamin Kanding, and Raghavendra Selvan. “CarbonTracker: Tracking and Predicting the Carbon Footprint of Training Deep Learning Models.” *arXiv*, 2020. https://doi.org/10.48550/arXiv.2007.03051

- Desislavov, Radosvet, Fernando Martínez-Plumed, and José Hernández-Orallo. “Compute and Energy Consumption Trends in Deep Learning Inference.” *arXiv*, 2021. https://doi.org/10.48550/arXiv.2109.05472

- Ricci Lara, María Agustina, Rodrigo Echeveste, and Enzo Ferrante. “Addressing Fairness in Artificial Intelligence for Medical Imaging.” *Nature Communications* 13, no. 1 (2022). https://doi.org/10.1038/s41467-022-32186-3

- Mugabowindekwe, Maurice, Martin Brandt, Jérôme Chave, Florian Reiner, David L. Skole, Ankit Kariryaa, Christian Igel, et al. “Nation-wide Mapping of Tree-Level Aboveground Carbon Stocks in Rwanda.” *Nature Climate Change* 13 (2023): 91–97. https://doi.org/10.1038/s41558-022-01544-w

- Settles, Burr. *Active Learning Literature Survey.* Computer Sciences Technical Report 1648. University of Wisconsin–Madison, 2009. https://burrsettles.com/pub/settles.activelearning.pdf

- Baldridge, Jason, and Miles Osborne. “Active Learning and the Total Cost of Annotation.” In *Proceedings of the 2004 Conference on Empirical Methods in Natural Language Processing*, 9–16. Barcelona: Association for Computational Linguistics, 2004. https://aclanthology.org/W04-3202/

- Toneva, Mariya, Alessandro Sordoni, Remi Tachet des Combes, Adam Trischler, Yoshua Bengio, and Geoffrey J. Gordon. “An Empirical Study of Example Forgetting during Deep Neural Network Learning.” *arXiv*, 2018. https://doi.org/10.48550/arXiv.1812.05159

- Guo, Chengcheng, Bo Zhao, and Yanbing Bai. “DeepCore: A Comprehensive Library for Coreset Selection in Deep Learning.” In *Database and Expert Systems Applications: 33rd International Conference, DEXA 2022, Vienna, Austria, August 22–24, 2022, Proceedings, Part I (LNCS 13108)*, 181–195. Cham: Springer, 2022. https://doi.org/10.1007/978-3-031-12423-5_14

- Yu, Ruonan, Songhua Liu, and Xinchao Wang. “Dataset Distillation: A Comprehensive Review.” *arXiv*, 2023. https://doi.org/10.48550/arXiv.2301.07014

- Zhao, Bo, and Hakan Bilen. “Dataset Condensation with Differentiable Siamese Augmentation.” *arXiv*, 2021. https://doi.org/10.48550/arXiv.2102.08259

- Wang, Tongzhou, Jun-Yan Zhu, Antonio Torralba, and Alexei A. Efros. “Dataset Distillation.” *arXiv*, 2018. https://doi.org/10.48550/arXiv.1811.10959

- Zhao, Bo, Konda Reddy Mopuri, and Hakan Bilen. “Dataset Condensation with Gradient Matching.” *arXiv*, 2020. https://doi.org/10.48550/arXiv.2006.05929

- Amid, Ehsan, Rohan Anil, Wojciech Kotłowski, and Manfred K. Warmuth. “Learning from Randomly Initialized Neural Network Features.” *arXiv*, 2022. https://doi.org/10.48550/arXiv.2202.06438

- Wolpert, David H., and William G. Macready. “No Free Lunch Theorems for Optimization.” *IEEE Transactions on Evolutionary Computation* 1, no. 1 (1997): 67–82. https://doi.org/10.1109/4235.585893

- Jin, Wei, Lingxiao Zhao, Shichang Zhang, Yozen Liu, Jiliang Tang, and Neil Shah. “Graph Condensation for Graph Neural Networks.” *arXiv*, 2021. https://doi.org/10.48550/arXiv.2110.07580


# Chapter 5: Automating Model Selection

# Chapter 6: Training Efficiency

# Chapter 7: Lean Inference

# Chapter 8: Hardware Considerations

# Chapter 9: A Recipe For Sustainable AI

# Chapter 10: Toward Sustainable AI

