# Bibliography for the book: Sustainable AI

Chapter-wise collection of bibliography.

1. [Sustainability and Artificial Intelligence](#sustainability-and-artificial-intelligence)
2. [Under the Hood of Generative AI](#under-the-hood-of-generative-ai)
3. [Quantifying the Efficiency of Deep Learning](#quantifying-the-efficiency-of-deep-learning)
4. [Data Parsimony](#data-parsimony)
5. [Automating Model Selection](#automating-model-selection)
6. [Training Efficiency](#training-efficiency)
7. [Lean Inference](#lean-inference)
8. [Hardware Considerations](#hardware-considerations)
9. [A Recipe for Sustainable AI](#a-recipe-for-sustainable-ai)
10. [Toward Sustainable AI](#toward-sustainable-ai)

## Sustainability and Artificial Intelligence

- Intergovernmental Panel on Climate Change (IPCC). *Climate Change 2023: Synthesis Report. Contribution of Working Groups I, II, and III to the Sixth Assessment Report of the Intergovernmental Panel on Climate Change.* Geneva: IPCC, 2023. https://doi.org/10.59327/IPCC/AR6-9789291691647

- World Commission on Environment and Development. *Our Common Future.* Oxford: Oxford University Press, 1987.

- Jantzen, Jan, Michael Kristensen, and Toke Haunstrup Christensen. “Sociotechnical Transition to Smart Energy: The Case of Samsø 1997–2030.” *Energy* 162 (August 3, 2018): 20–34. https://doi.org/10.1016/j.energy.2018.07.174

- United Nations Framework Convention on Climate Change (UNFCCC). “Samsø: An Island Community Pointing to the Future.” 2023. https://unfccc.int/climate-action/un-global-climate-action-awards/climate-leaders/samso

- McCarthy, John, Marvin Minsky, Nathaniel Rochester, and Claude E. Shannon. “Dartmouth Summer Research Project on Artificial Intelligence.” 1956. Related proposal DOI: https://doi.org/10.1609/aimag.v27i4.1904

- Schmidhuber, Juergen. “Annotated History of Modern AI and Deep Learning.” *arXiv.org*, December 21, 2022. https://doi.org/10.48550/arXiv.2212.11279

- Lynch, Shana. “Andrew Ng: Why AI Is the New Electricity.” Stanford Graduate School of Business, March 11, 2017. https://www.gsb.stanford.edu/insights/andrew-ng-why-ai-new-electricity

- Pasquinelli, Matteo. *The Eye of the Master: A Social History of Artificial Intelligence.* London: Verso Books, 2023.

- Murgia, Madhumita, and Nathalie Thomas. “DeepMind and National Grid in AI Talks to Balance Energy Supply.” *Financial Times*, March 11, 2017. https://www.ft.com/content/27c8aea0-06a9-11e7-97d1-5e720a26771b

- Meister, Miriam. “AI Predicts Flooding.” Technical University of Denmark, March 7, 2024. https://www.dtu.dk/english/newsarchive/2024/03/ai-predicts-flooding

- Nearing, Grey, Deborah Cohen, Vusumuzi Dube, Martin Gauch, Oren Gilon, Shaun Harrigan, Avinatan Hassidim, et al. “Global Prediction of Extreme Floods in Ungauged Watersheds.” *Nature* 627 (March 20, 2024): 559–63. https://doi.org/10.1038/s41586-024-07145-1

- Ziemba, Ewa Wanda, Cong Doanh Duong, Joanna Ejdys, Maria Alejandra Gonzalez-Perez, Ruta Kazlauskaitė, Pawel Korzynski, Grzegorz Mazurek, Joanna Paliszkiewicz, Jelena Stankevičienė, and Krzysztof Wach. “Leveraging Artificial Intelligence to Meet the Sustainable Development Goals.” *Journal of Economics and Management* 46 (December 16, 2024): 508–83. https://doi.org/10.22367/jem.2024.46.19

- Yeh, Christopher, Anthony Perez, Anne Driscoll, George Azzari, Zhongyi Tang, David Lobell, Stefano Ermon, and Marshall Burke. “Using Publicly Available Satellite Imagery and Deep Learning to Understand Economic Well-Being in Africa.” *Nature Communications* 11 (May 22, 2020). https://doi.org/10.1038/s41467-020-16185-w

- McKinney, Scott Mayer, Marcin Sieniek, Varun Godbole, Jonathan Godwin, Natasha Antropova, Hutan Ashrafian, Trevor Back, et al. “International Evaluation of an AI System for Breast Cancer Screening.” *Nature* 577, no. 7788 (January 1, 2020): 89–94. https://doi.org/10.1038/s41586-019-1799-6

- Oladapo, Bankole I., Mattew A. Olawumi, and Francis T. Omigbodun. “Machine Learning for Optimising Renewable Energy and Grid Efficiency.” *Atmosphere* 15, no. 10 (October 19, 2024): 1250. https://doi.org/10.3390/atmos15101250

- Paolo, Fernando S., David Kroodsma, Jennifer Raynor, Tim Hochberg, Pete Davis, Jesse Cleary, Luca Marsaglia, Sara Orofino, Christian Thomas, and Patrick Halpin. “Satellite Mapping Reveals Extensive Industrial Activity at Sea.” *Nature* 625, no. 7993 (January 3, 2024): 85–91. https://doi.org/10.1038/s41586-023-06825-8

- Morozov, Evgeny. *To Save Everything, Click Here: Technology, Solutionism and the Urge to Fix Problems That Don’t Exist.* Review in *Information Polity* 18, no. 3 (July 1, 2013): 275–76. (No DOI available.)

- Brevini, Benedetta. “Myths, Techno-Solutionism and Artificial Intelligence: Reclaiming AI Materiality and Its Massive Environmental Costs.” In *Handbook of Critical Studies of Artificial Intelligence*, edited by Simon Lindgren, 869–77. Cheltenham, UK: Edward Elgar Publishing, 2023. https://doi.org/10.4337/9781803928562.00086

- Kaplan, Jared, Sam McCandlish, Tom Henighan, Tom B. Brown, Benjamin Chess, Rewon Child, Scott Gray, et al. “Scaling Laws for Neural Language Models.” *arXiv.org*, January 23, 2020. https://doi.org/10.48550/arXiv.2001.08361

- Bakhtiarifard, Pedram, Pınar Tözün, Christian Igel, and Raghavendra Selvan. “Climate and Resource Awareness Is Imperative to Achieving Sustainable AI (and Preventing a Global AI Arms Race).” *arXiv*, February 27, 2025. https://doi.org/10.48550/arXiv.2502.20016

- International Energy Agency (IEA). *Energy and AI.* Paris: IEA, 2025. https://www.iea.org/reports/energy-and-ai

- International Energy Agency (IEA). *Electricity 2025.* Paris: IEA, 2025. https://www.iea.org/reports/electricity-2025

- Bickerstaff, Karen, Gordon Walker, and Harriet Bulkeley, eds. *Energy Justice in a Changing Climate: Social Equity and Low-Carbon Energy.* London: Zed Books, 2013.

- Li, Pengfei, Jianyi Yang, Mohammad A. Islam, and Shaolei Ren. “Making AI Less ‘Thirsty’: Uncovering and Addressing the Secret Water Footprint of AI Models.” *Communications of the ACM* 68, no. 7 (2025): 54–61. https://doi.org/10.1145/3724499

- Ministry of Jal Shakti (India). *Annual Report 2024–25.* Government of India, 2025. https://www.jalshakti-dowr.gov.in/static/uploads/2024/05/fc00cd887135cf39b2005ccf1539e0e5.pdf

- Tan, Eli, and Dustin Chambers. “Their Water Taps Ran Dry When Meta Built Next Door.” *New York Times*, July 14, 2025. https://www.nytimes.com/2025/07/14/technology/meta-data-center-water.html

- Kokkegård, Hanne. “Utilize Waste Heat From Data Centers in District Heating.” Technical University of Denmark, November 1, 2022. https://www.dtu.dk/english/news/all-news/nyhed?id=b2e4c8f0-0c62-437d-afbf-d2a17c376ffa

- Temple, James. “The Data Center Boom in the Desert.” *MIT Technology Review*, May 20, 2025. https://www.technologyreview.com/2025/05/20/1116287/ai-data-centers-nevada-water-reno-computing-environmental-impact/

- Luccioni, Alexandra Sasha, Sylvain Viguier, and Anne-Laure Ligozat. “Estimating the Carbon Footprint of BLOOM, a 176B Parameter Language Model.” *Journal of Machine Learning Research* 24, no. 1 (2023): 11990–12004. https://www.jmlr.org/papers/v24/23-0069.html

- Giglio, Elena. “Extractivism and Its Socio-Environmental Impact in South America. Overview of the ‘Lithium Triangle.’” *América Crítica* 5, no. 1 (2021): 47–53. https://doi.org/10.13125/americacritica/4926

- Schwartz, Roy, Jesse Dodge, Noah A. Smith, and Oren Etzioni. “Green AI.” *Communications of the ACM* 63, no. 12 (2020): 54–63. https://doi.org/10.1145/3381831

- Khazzoom, J. Daniel. “Economic Implications of Mandated Efficiency in Standards for Household Appliances.” *The Energy Journal* 1, no. 4 (1980): 21–40. https://doi.org/10.5547/issn0195-6574-ej-vol1-no4-2

- Wright, Dustin, Christian Igel, Gabrielle Samuel, and Raghavendra Selvan. “Efficiency Is Not Enough: A Critical Perspective of Environmentally Sustainable AI.” *Communications of the ACM* 68, no. 7 (2025): 62–69. https://doi.org/10.1145/3724500

## Under the Hood of Generative AI

- Brown, Tom B., Benjamin Mann, Nick Ryder, Melanie Subbiah, Jared D. Kaplan, Prafulla Dhariwal, Arvind Neelakantan, et al. “Language Models Are Few-Shot Learners.” *arXiv*, May 28, 2020. https://doi.org/10.48550/arXiv.2005.14165

- Rombach, Robin, Andreas Blattmann, Dominik Lorenz, Patrick Esser, and Björn Ommer. “High-Resolution Image Synthesis with Latent Diffusion Models.” *arXiv*, December 20, 2021. https://doi.org/10.48550/arXiv.2112.10752

- Agostinelli, Andrea, Timo I. Denk, Zalán Borsos, Jesse Engel, Mauro Verzetti, Antoine Caillon, Qingqing Huang, et al. “MusicLM: Generating Music from Text.” *arXiv*, January 26, 2023. https://doi.org/10.48550/arXiv.2301.11325

- Bender, Emily M. “Resisting Dehumanization in the Age of ‘AI.’” *Current Directions in Psychological Science* 33, no. 2 (2024): 114–120. https://doi.org/10.1177/09637214231217286

- Bakhtiarifard, Pedram, Pınar Tözün, Christian Igel, and Raghavendra Selvan. “Climate and Resource Awareness Is Imperative to Achieving Sustainable AI (and Preventing a Global AI Arms Race).” *arXiv*, February 27, 2025. https://doi.org/10.48550/arXiv.2502.20016

- Bengio, Yoshua, Aaron Courville, and Pascal Vincent. “Representation Learning: A Review and New Perspectives.” *arXiv*, June 24, 2012. https://doi.org/10.48550/arXiv.1206.5538

- Gonzalez, Rafael C., and Richard E. Woods. *Digital Image Processing*. 2nd ed. Reading, MA: Addison-Wesley, 1987.

- Kramer, Mark A. “Nonlinear Principal Component Analysis Using Autoassociative Neural Networks.” *AIChE Journal* 37, no. 2 (1991): 233–243. https://doi.org/10.1002/aic.690370209

- Goodfellow, Ian, Yoshua Bengio, and Aaron Courville. “Autoencoders.” In *Deep Learning*. Cambridge, MA: MIT Press, 2016. https://www.deeplearningbook.org/contents/autoencoders.html

- Kingma, Diederik P., and Max Welling. “Auto-Encoding Variational Bayes.” *arXiv*, December 20, 2013. https://doi.org/10.48550/arXiv.1312.6114

- Radford, Alec, Jong Wook Kim, Chris Hallacy, Aditya Ramesh, Gabriel Goh, Sandhini Agarwal, Girish Sastry, et al. “Learning Transferable Visual Models from Natural Language Supervision.” *Proceedings of the 38th International Conference on Machine Learning (ICML 2021)*, 8748–8763. Also *arXiv*, February 26, 2021. https://doi.org/10.48550/arXiv.2103.00020

- Rosenblatt, Frank. “The Perceptron: A Probabilistic Model for Information Storage and Organization in the Brain.” *Psychological Review* 65, no. 6 (1958): 386–408. https://doi.org/10.1037/h0042519

- Rumelhart, David E., Geoffrey E. Hinton, and Ronald J. Williams. “Learning Representations by Back-Propagating Errors.” *Nature* 323 (October 9, 1986): 533–536. https://doi.org/10.1038/323533a0

- LeCun, Yann, Léon Bottou, Yoshua Bengio, and Patrick Haffner. “Gradient-Based Learning Applied to Document Recognition.” *Proceedings of the IEEE* 86, no. 11 (1998): 2278–2324. https://doi.org/10.1109/5.726791

- Elman, Jeffrey L. “Finding Structure in Time.” *Cognitive Science* 14, no. 2 (1990): 179–211. https://doi.org/10.1207/s15516709cog1402_1

- Hochreiter, Sepp, and Jürgen Schmidhuber. “Long Short-Term Memory.” *Neural Computation* 9, no. 8 (1997): 1735–1780. https://doi.org/10.1162/neco.1997.9.8.1735

- Kipf, Thomas N., and Max Welling. “Semi-Supervised Classification with Graph Convolutional Networks.” *arXiv*, September 9, 2016. https://doi.org/10.48550/arXiv.1609.02907

- Vaswani, Ashish, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones, Aidan N. Gomez, Łukasz Kaiser, and Illia Polosukhin. “Attention Is All You Need.” *arXiv*, June 12, 2017. https://doi.org/10.48550/arXiv.1706.03762

- Duman Keles, Feyza, Pruthuvi Mahesakya Wijewardena, and Chinmay Hegde. “On the Computational Complexity of Self-Attention.” *arXiv*, September 11, 2022. https://arxiv.org/abs/2209.04881

- Grefenstette, Gregory. “Tokenization.” In *Syntactic Wordclass Tagging*, edited by H. van Halteren, 117–133. Dordrecht: Springer, 1999. https://doi.org/10.1007/978-94-015-9273-4_9

- Ruder, Sebastian. “An Overview of Gradient Descent Optimization Algorithms.” *arXiv*, September 15, 2016. https://doi.org/10.48550/arXiv.1609.04747

- Baydin, Atilim Gunes, Barak A. Pearlmutter, Alexey Andreyevich Radul, and Jeffrey Mark Siskind. “Automatic Differentiation in Machine Learning: A Survey.” *Journal of Machine Learning Research* 18 (2018): 1–43. Preprint *arXiv*, February 20, 2015. https://doi.org/10.48550/arXiv.1502.05767

- Kingma, Diederik P., and Jimmy Ba. “Adam: A Method for Stochastic Optimization.” *arXiv*, December 22, 2014. https://doi.org/10.48550/arXiv.1412.6980

- Lex Clips. “Transformers: The Best Idea in AI | Andrej Karpathy and Lex Fridman.” YouTube video, November 1, 2022. https://youtu.be/9uw3F6rndnA?t=393

- Sevilla, Jaime, Lennart Heim, Anson Ho, Tamay Besiroglu, Marius Hobbhahn, and Pablo Villalobos. “Compute Trends Across Three Eras of Machine Learning.” *2022 International Joint Conference on Neural Networks (IJCNN)*, July 18, 2022: 1–8. https://doi.org/10.1109/IJCNN55064.2022.9891914. See also the preprint: *arXiv*, February 11, 2022. https://arxiv.org/abs/2202.05924

## Quantifying the Efficiency of Deep Learning

- Sevilla, Jaime, Lennart Heim, Anson Ho, Tamay Besiroglu, Marius Hobbhahn, and Pablo Villalobos. “Compute Trends Across Three Eras of Machine Learning.” *Proceedings of the 2022 International Joint Conference on Neural Networks (IJCNN)*, July 18, 2022, 1–8. https://doi.org/10.1109/IJCNN55064.2022.9891914

- Wright, Dustin, Christian Igel, Gabrielle Samuel, and Raghavendra Selvan. “Efficiency Is Not Enough: A Critical Perspective of Environmentally Sustainable AI.” *Communications of the ACM* 68, no. 7 (2025): 62–69. https://doi.org/10.1145/3724500

- Menabrea, L. F. “Sketch of the Analytical Engine Invented by Charles Babbage, Esq.” *Bibliothèque Universelle de Genève* (October 1842). English translation with notes by Ada Lovelace, 1843. http://psychclassics.yorku.ca/Lovelace/menabrea.htm

- Intergovernmental Panel on Climate Change (IPCC). *Climate Change 2014: Mitigation of Climate Change. Contribution of Working Group III to the Fifth Assessment Report of the Intergovernmental Panel on Climate Change.* Cambridge: Cambridge University Press, 2014. https://doi.org/10.1017/CBO9781107415416

- Freitag, Charlotte, Mike Berners-Lee, Kelly Widdicks, Bran Knowles, Gordon S. Blair, and Adrian Friday. “The Real Climate and Transformative Impact of ICT: A Critique of Estimates, Trends, and Regulations.” *Patterns* 2, no. 9 (2021): 100340. https://doi.org/10.1016/j.patter.2021.100340

- Purvis, Ben, Yong Mao, and Darren Robinson. “Three Pillars of Sustainability: In Search of Conceptual Origins.” *Sustainability Science* 14, no. 3 (2019): 681–695. https://doi.org/10.1007/s11625-018-0627-5

- Strassen, Volker. “Gaussian Elimination Is Not Optimal.” *Numerische Mathematik* 13, no. 4 (1969): 354–356. https://doi.org/10.1007/BF02165411

- Fawzi, Alhussein, Matej Balog, Aja Huang, Thomas Hubert, Bernardino Romera-Paredes, Mohammadamin Barekatain, Alexander Novikov, et al. “Discovering Faster Matrix Multiplication Algorithms with Reinforcement Learning.” *Nature* 610, no. 7930 (2022): 47–53. https://doi.org/10.1038/s41586-022-05172-4

- Ahmed, Nur, and Muntasir Wahed. “The De-democratization of AI: Deep Learning and the Compute Divide in Artificial Intelligence Research.” *arXiv* (October 22, 2020). https://doi.org/10.48550/arXiv.2010.15581

- Dodge, Jesse, Taylor Prewitt, Remi Tachet des Combes, Erika Odmark, Roy Schwartz, Emma Strubell, Alexandra Sasha Luccioni, Noah A. Smith, Nicole DeCario, and Will Buchanan. “Measuring the Carbon Intensity of AI in Cloud Instances.” In *Proceedings of the 2022 ACM Conference on Fairness, Accountability, and Transparency (FAccT ’22)*, June 20–24, 2022, 1877–1894. https://doi.org/10.1145/3531146.3533234

- Hwang, Tim. “Computational Power and the Social Impact of Artificial Intelligence.” *arXiv* (March 23, 2018). https://doi.org/10.48550/arXiv.1803.08971

- Strubell, Emma, Ananya Ganesh, and Andrew McCallum. “Energy and Policy Considerations for Deep Learning in NLP.” *arXiv* (June 5, 2019). https://doi.org/10.48550/arXiv.1906.02243

- Polimeni, John M., Kozo Mayumi, Mario Giampietro, and Blake Alcott. *The Myth of Resource Efficiency: The Jevons Paradox.* London: Earthscan, 2008.

- Radovanovic, Ana, Ross Koningstein, Ian Schneider, Bokan Chen, Alexandre Duarte, Binz Roy, Diyue Xiao, et al. “Carbon-Aware Computing for Datacenters.” *arXiv* (June 11, 2021). https://doi.org/10.48550/arXiv.2106.11750

- Anthony, Lasse F. Wolff, Benjamin Kanding, and Raghavendra Selvan. “Carbontracker: Tracking and Predicting the Carbon Footprint of Training Deep Learning Models.” *arXiv* (July 6, 2020). https://doi.org/10.48550/arXiv.2007.03051

- Bouza, Lucía, Aurélie Bugeau, and Loïc Lannelongue. “How to Estimate Carbon Footprint When Training Deep Learning Models? A Guide and Review.” *Environmental Research Communications* 5, no. 11 (2023): 115014. https://doi.org/10.1088/2515-7620/acf81b

- Wolpert, David H., and William G. Macready. “No Free Lunch Theorems for Optimization.” *IEEE Transactions on Evolutionary Computation* 1, no. 1 (1997): 67–82. https://doi.org/10.1109/4235.585893

- Patterson, David, Joseph Gonzalez, Urs Hölzle, Quoc Le, Chen Liang, Lluis-Miquel Munguia, Daniel Rothchild, David So, Maud Texier, and Jeff Dean. “The Carbon Footprint of Machine Learning Training Will Plateau, Then Shrink.” *arXiv* (April 11, 2022). https://doi.org/10.48550/arXiv.2204.05149


## Data Parsimony

- Grattafiori, Aaron, Abhimanyu Dubey, Abhinav Jauhri, Abhinav Pandey, Abhishek Kadian, Ahmad Al-Dahle, Aiesha Letman, et al. “The Llama 3 Herd of Models.” *arXiv*, 2024. https://doi.org/10.48550/arXiv.2407.21783

- International Energy Agency. “Data Centres and Data Transmission Networks.” IEA, 2023. https://www.iea.org/analyses/data-centres-and-data-transmission-networks

- Freitag, Charlotte, Mike Berners-Lee, Kelly Widdicks, Bran Knowles, Gordon S. Blair, and Adrian Friday. “The Climate Impact of ICT: A Review of Estimates, Trends and Regulations.” *arXiv*, 2021. https://doi.org/10.48550/arXiv.2102.02622

- Seagate Technology LLC. *Enterprise Capacity 3.5 HDD: v5.1 Product Manual.* September 2017. https://www.seagate.com/files/www-content/product-content/enterprise-hdd-fam/enterprise-capacity-3-5-hdd/enterprise-capacity-3-5-hdd/en-us/docs/100805922c.pdf

- Smith, Shaden, Mostofa Patwary, Brandon Norick, Patrick LeGresley, Samyam Rajbhandari, Jared Casper, Zhun Liu, et al. “Using DeepSpeed and Megatron to Train Megatron-Turing NLG 530B, a Large-Scale Generative Language Model.” *arXiv*, 2022. https://doi.org/10.48550/arXiv.2201.11990

- Hoffmann, Jordan, Sebastian Borgeaud, Arthur Mensch, Elena Buchatskaya, Trevor Cai, Eliza Rutherford, Diego de Las Casas, et al. “Training Compute-Optimal Large Language Models.” *arXiv*, 2022. https://doi.org/10.48550/arXiv.2203.15556

- Villalobos, Pablo, and Anson Ho. *Trends in Training Dataset Sizes.* Epoch AI, 2022. https://epoch.ai/blog/trends-in-training-dataset-sizes

- Anthony, Lasse F. Wolff, Benjamin Kanding, and Raghavendra Selvan. “CarbonTracker: Tracking and Predicting the Carbon Footprint of Training Deep Learning Models.” *arXiv*, 2020. https://doi.org/10.48550/arXiv.2007.03051

- Desislavov, Radosvet, Fernando Martínez-Plumed, and José Hernández-Orallo. “Compute and Energy Consumption Trends in Deep Learning Inference.” *arXiv*, 2021. https://doi.org/10.48550/arXiv.2109.05472

- Ricci Lara, María Agustina, Rodrigo Echeveste, and Enzo Ferrante. “Addressing Fairness in Artificial Intelligence for Medical Imaging.” *Nature Communications* 13, no. 1 (2022). https://doi.org/10.1038/s41467-022-32186-3

- Mugabowindekwe, Maurice, Martin Brandt, Jérôme Chave, Florian Reiner, David L. Skole, Ankit Kariryaa, Christian Igel, et al. “Nation-wide Mapping of Tree-Level Aboveground Carbon Stocks in Rwanda.” *Nature Climate Change* 13 (2023): 91–97. https://doi.org/10.1038/s41558-022-01544-w

- Settles, Burr. *Active Learning Literature Survey.* Computer Sciences Technical Report 1648. University of Wisconsin–Madison, 2009. https://burrsettles.com/pub/settles.activelearning.pdf

- Baldridge, Jason, and Miles Osborne. “Active Learning and the Total Cost of Annotation.” In *Proceedings of the 2004 Conference on Empirical Methods in Natural Language Processing*, 9–16. Barcelona: Association for Computational Linguistics, 2004. https://aclanthology.org/W04-3202/

- Toneva, Mariya, Alessandro Sordoni, Remi Tachet des Combes, Adam Trischler, Yoshua Bengio, and Geoffrey J. Gordon. “An Empirical Study of Example Forgetting during Deep Neural Network Learning.” *arXiv*, 2018. https://doi.org/10.48550/arXiv.1812.05159

- Guo, Chengcheng, Bo Zhao, and Yanbing Bai. “DeepCore: A Comprehensive Library for Coreset Selection in Deep Learning.” In *Database and Expert Systems Applications: 33rd International Conference, DEXA 2022, Vienna, Austria, August 22–24, 2022, Proceedings, Part I (LNCS 13108)*, 181–195. Cham: Springer, 2022. https://doi.org/10.1007/978-3-031-12423-5_14

- Yu, Ruonan, Songhua Liu, and Xinchao Wang. “Dataset Distillation: A Comprehensive Review.” *arXiv*, 2023. https://doi.org/10.48550/arXiv.2301.07014

- Zhao, Bo, and Hakan Bilen. “Dataset Condensation with Differentiable Siamese Augmentation.” *arXiv*, 2021. https://doi.org/10.48550/arXiv.2102.08259

- Wang, Tongzhou, Jun-Yan Zhu, Antonio Torralba, and Alexei A. Efros. “Dataset Distillation.” *arXiv*, 2018. https://doi.org/10.48550/arXiv.1811.10959

- Zhao, Bo, Konda Reddy Mopuri, and Hakan Bilen. “Dataset Condensation with Gradient Matching.” *arXiv*, 2020. https://doi.org/10.48550/arXiv.2006.05929

- Amid, Ehsan, Rohan Anil, Wojciech Kotłowski, and Manfred K. Warmuth. “Learning from Randomly Initialized Neural Network Features.” *arXiv*, 2022. https://doi.org/10.48550/arXiv.2202.06438

- Wolpert, David H., and William G. Macready. “No Free Lunch Theorems for Optimization.” *IEEE Transactions on Evolutionary Computation* 1, no. 1 (1997): 67–82. https://doi.org/10.1109/4235.585893

- Jin, Wei, Lingxiao Zhao, Shichang Zhang, Yozen Liu, Jiliang Tang, and Neil Shah. “Graph Condensation for Graph Neural Networks.” *arXiv*, 2021. https://doi.org/10.48550/arXiv.2110.07580


## Automating Model Selection

- Zoph, Barret, and Quoc V. Le. “Neural Architecture Search with Reinforcement Learning.” *arXiv*, November 5, 2016. https://doi.org/10.48550/arXiv.1611.01578

- He, Kaiming, Xiangyu Zhang, Shaoqing Ren, and Jian Sun. “Deep Residual Learning for Image Recognition.” *arXiv*, December 10, 2015. https://doi.org/10.48550/arXiv.1512.03385

- Ying, Chris, Aaron Klein, Esteban Real, Eric Christiansen, Kevin Murphy, and Frank Hutter. “NAS-Bench-101: Towards Reproducible Neural Architecture Search.” *arXiv*, February 25, 2019. https://doi.org/10.48550/arXiv.1902.09635

- Elsken, Thomas, Jan Hendrik Metzen, and Frank Hutter. “Neural Architecture Search: A Survey.” *Journal of Machine Learning Research* 20 (2019): 1–21. http://www.jmlr.org/papers/volume20/18-598/18-598.pdf

- Bakhtiarifard, Pedram, Christian Igel, and Raghavendra Selvan. “EC-NAS: Energy Consumption Aware Tabular Benchmarks for Neural Architecture Search.” In *ICASSP 2024 – IEEE International Conference on Acoustics, Speech and Signal Processing*, 5660–5664. April 2024. https://doi.org/10.1109/ICASSP48485.2024.10448303

- Dai, Xiaoliang, Alvin Wan, Peizhao Zhang, Bichen Wu, Zijian He, Zhen Wei, Kan Chen, et al. “FBNetV3: Joint Architecture-Recipe Search Using Predictor Pretraining.” *arXiv*, June 3, 2020. https://doi.org/10.48550/arXiv.2006.02049

- Wang, Yuyang, Zijie Li, and Amir Barati Farimani. “Graph Neural Networks for Molecules.” In *Machine Learning in Molecular Sciences*, edited by Chen Qu and Hanchao Liu, 21–66. Cham: Springer, 2023. https://doi.org/10.1007/978-3-031-37196-7_2

- Mockus, J. B., and L. J. Mockus. “Bayesian Approach to Global Optimization and Application to Multiobjective and Constrained Problems.” *Journal of Optimization Theory and Applications* 70, no. 1 (1991): 157–172. https://doi.org/10.1007/BF00940509

- Martinez-Cantin, Ruben. “BayesOpt: A Bayesian Optimization Library for Nonlinear Optimization, Experimental Design and Bandits.” *Journal of Machine Learning Research* 15 (2014): 3915–3919. https://www.jmlr.org/papers/volume15/martinezcantin14a/martinezcantin14a.pdf

- Song, Jiaming, Lantao Yu, Willie Neiswanger, and Stefano Ermon. “A General Recipe for Likelihood-Free Bayesian Optimization.” In *Proceedings of the 39th International Conference on Machine Learning (PMLR 162)*, 20384–20404, 2022. https://proceedings.mlr.press/v162/song22b.html

- Cai, Han, Chuang Gan, Tianzhe Wang, Zhekai Zhang, and Song Han. “Once-for-All: Train One Network and Specialize It for Efficient Deployment.” *arXiv*, August 26, 2019. https://doi.org/10.48550/arXiv.1908.09791

- Zhao, Yiyang, Yunzhuo Liu, Bo Jiang, and Tian Guo. “CE-NAS: An End-to-End Carbon-Efficient Neural Architecture Search Framework.” *arXiv*, June 3, 2024. https://doi.org/10.48550/arXiv.2406.01414

- Zhou, Kaixiong, Qingquan Song, Xiao Huang, and Xia Hu. “Auto-GNN: Neural Architecture Search of Graph Neural Networks.” *arXiv*, September 7, 2019. https://doi.org/10.48550/arXiv.1909.03184

- Vaswani, Ashish, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones, Aidan N. Gomez, Łukasz Kaiser, and Illia Polosukhin. “Attention Is All You Need.” *arXiv*, June 12, 2017. https://doi.org/10.48550/arXiv.1706.03762

- Frankle, Jonathan, and Michael Carbin. “The Lottery Ticket Hypothesis: Finding Sparse, Trainable Neural Networks.” *arXiv*, March 9, 2018. https://doi.org/10.48550/arXiv.1803.03635

- Yang, Greg, Edward J. Hu, Igor Babuschkin, Szymon Sidor, Xiaodong Liu, David Farhi, Nick Ryder, Jakub Pachocki, Weizhu Chen, and Jianfeng Gao. “Tensor Programs V: Tuning Large Neural Networks via Zero-Shot Hyperparameter Transfer.” *arXiv*, March 7, 2022. https://doi.org/10.48550/arXiv.2203.03466

- Shazeer, Noam, Azalia Mirhoseini, Krzysztof Maziarz, Andy Davis, Quoc Le, Geoffrey Hinton, and Jeff Dean. “Outrageously Large Neural Networks: The Sparsely-Gated Mixture-of-Experts Layer.” *arXiv*, January 23, 2017. https://doi.org/10.48550/arXiv.1701.06538

- Davis, Andrew, and Itamar Arel. “Low-Rank Approximations for Conditional Feedforward Computation in Deep Neural Networks.” *arXiv*, December 16, 2013. https://doi.org/10.48550/arXiv.1312.4461

- Klein, Aaron, and Frank Hutter. “Tabular Benchmarks for Joint Architecture and Hyperparameter Optimization.” *arXiv*, May 13, 2019. https://doi.org/10.48550/arXiv.1905.04970

- Tan, Mingxing, and Quoc V. Le. “EfficientNet: Rethinking Model Scaling for Convolutional Neural Networks.” *arXiv*, May 28, 2019. https://doi.org/10.48550/arXiv.1905.11946

- Dooley, Samuel, Rhea Sanjay Sukthanker, John P. Dickerson, Colin White, Frank Hutter, and Micah Goldblum. “Rethinking Bias Mitigation: Fairer Architectures Make for Fairer Face Recognition.” In *Advances in Neural Information Processing Systems 36 (NeurIPS 2023)*. 2023. https://papers.nips.cc/paper_files/paper/2023/hash/eb3c42ddfa16d8421fdba13528107cc1-Abstract-Conference.html


## Training Efficiency

- Avelar, Victor, Patrick Donovan, Paul Lin, Wendy Torell, and Maria A. Torres Arango. “The AI Disruption: Challenges and Guidance for Data Center Design.” Schneider Electric White Paper 110 (Version 1.1), 2023. https://www.se.com/us/en/download/document/SPD_WP110_EN/

- Bozinovski, Stevo, and Ante Fulgosi. “The Influence of Pattern Similarity and Transfer of Learning upon Training of a Base Perceptron B2.” In *Proceedings of the Symposium Informatica*, Bled, 1976. https://www.informatica.si/index.php/informatica/article/view/2828

- Ganin, Yaroslav, and Victor Lempitsky. “Unsupervised Domain Adaptation by Backpropagation.” *arXiv*, 2014. https://doi.org/10.48550/arXiv.1409.7495

- Brown, Tom B., Benjamin Mann, Nick Ryder, Melanie Subbiah, Jared D. Kaplan, Prafulla Dhariwal, Arvind Neelakantan, et al. “Language Models Are Few-Shot Learners.” *arXiv*, 2020. https://doi.org/10.48550/arXiv.2005.14165

- Grattafiori, Aaron, Abhimanyu Dubey, Abhinav Jauhri, Abhinav Pandey, Abhishek Kadian, Ahmad Al-Dahle, Aiesha Letman, et al. “The Llama 3 Herd of Models.” *arXiv*, 2024. https://doi.org/10.48550/arXiv.2407.21783

- Han, Song, Jeff Pool, John Tran, and William J. Dally. “Learning Both Weights and Connections for Efficient Neural Networks.” *arXiv*, 2015. https://doi.org/10.48550/arXiv.1506.02626

- LeCun, Yann, John S. Denker, and Sara A. Solla. “Optimal Brain Damage.” In *Advances in Neural Information Processing Systems 2 (NeurIPS 1989)*. https://proceedings.neurips.cc/paper/1989/hash/6c9882bbac1c7093bd25041881277658-Abstract.html

- Anwar, Sajid, Kyuyeon Hwang, and Wonyong Sung. “Structured Pruning of Deep Convolutional Neural Networks.” *arXiv*, 2015. https://doi.org/10.48550/arXiv.1512.08571

- Golub, Gene H., and Charles F. Van Loan. *Matrix Computations*. 3rd ed. Baltimore: Johns Hopkins University Press, 1996.

- Martens, James, and Roger Grosse. “Optimizing Neural Networks with Kronecker-Factored Approximate Curvature.” *arXiv*, 2015. https://doi.org/10.48550/arXiv.1503.05671

- Novikov, Alexander, Dmitry Podoprikhin, Anton Osokin, and Dmitry Vetrov. “Tensorizing Neural Networks.” *arXiv*, 2015. https://doi.org/10.48550/arXiv.1509.06569

- Ren, Yuxin, Benyou Wang, Lifeng Shang, Xin Jiang, and Qun Liu. “Exploring Extreme Parameter Compression for Pre-trained Language Models.” *arXiv*, 2022. https://doi.org/10.48550/arXiv.2205.10036

- Thorsteinsson, Hallgrimur, Valdemar J. Henriksen, Tong Chen, and Raghavendra Selvan. “Adversarial Fine-Tuning of Compressed Neural Networks for Joint Improvement of Robustness and Efficiency.” *arXiv*, 2024. https://doi.org/10.48550/arXiv.2403.09441

- Hu, Edward J., Yelong Shen, Phillip Wallis, Zeyuan Allen-Zhu, Yuanzhi Li, Shean Wang, Lu Wang, and Weizhu Chen. “LoRA: Low-Rank Adaptation of Large Language Models.” *arXiv*, 2021. https://doi.org/10.48550/arXiv.2106.09685

- IEEE. *IEEE Standard for Floating-Point Arithmetic (IEEE Std 754-2019).* 2019. https://doi.org/10.1109/IEEESTD.2019.8766229

- Micikevicius, Paulius, Dušan Stošić, Neil Burgess, Marius Cornea, Pradeep Dubey, Richard Grisenthwaite, Sangwon Ha, et al. “FP8 Formats for Deep Learning.” *arXiv*, 2022. https://doi.org/10.48550/arXiv.2209.05433

- Dettmers, Tim, and Luke Zettlemoyer. “The Case for 4-Bit Precision: k-Bit Inference Scaling Laws.” *arXiv*, 2022. https://doi.org/10.48550/arXiv.2212.09720

- PyTorch Foundation. “Introducing Native PyTorch Automatic Mixed Precision for Faster Training on NVIDIA GPUs.” PyTorch Blog, July 28, 2020. https://pytorch.org/blog/accelerating-training-on-nvidia-gpus-with-pytorch-automatic-mixed-precision/

- Kingma, Diederik P., and Jimmy Ba. “Adam: A Method for Stochastic Optimization.” *arXiv*, 2014. https://doi.org/10.48550/arXiv.1412.6980

- Selvan, Raghavendra, Julian Schön, and Erik B. Dam. “Operating Critical Machine Learning Models in Resource Constrained Regimes.” *arXiv*, 2023. https://doi.org/10.48550/arXiv.2303.10181

- Dettmers, Tim, Mike Lewis, Sam Shleifer, and Luke Zettlemoyer. “8-Bit Optimizers via Block-Wise Quantization.” *arXiv*, 2021. https://doi.org/10.48550/arXiv.2110.02861

- AskariHemmat, MohammadHossein, Ahmadreza Jeddi, Reyhane Askari Hemmat, Ivan Lazarevich, Alexander Hoffman, Sudhakar Sah, Ehsan Saboori, Yvon Savaria, and Jean-Pierre David. “QGen: On the Ability to Generalize in Quantization Aware Training.” *arXiv*, 2024. https://doi.org/10.48550/arXiv.2404.11769

- Jacob, Benoit, Skirmantas Kligys, Bo Chen, Menglong Zhu, Matthew Tang, Andrew Howard, Hartwig Adam, and Dmitry Kalenichenko. “Quantization and Training of Neural Networks for Efficient Integer-Arithmetic-Only Inference.” *arXiv*, 2017. https://doi.org/10.48550/arXiv.1712.05877

- Loeschcke, Sebastian, Mads Toftrup, Michael J. Kastoryano, Serge Belongie, and Vésteinn Snæbjarnarson. “LoQT: Low-Rank Adapters for Quantized Pretraining.” *NeurIPS 2024*. Preprint *arXiv*, 2024. https://doi.org/10.48550/arXiv.2405.16528

- Dettmers, Tim, Artidoro Pagnoni, Ari Holtzman, and Luke Zettlemoyer. “QLoRA: Efficient Finetuning of Quantized LLMs.” *arXiv*, 2023. https://doi.org/10.48550/arXiv.2305.14314

- Kirkpatrick, James, Razvan Pascanu, Neil Rabinowitz, Joel Veness, Guillaume Desjardins, Andrei A. Rusu, Kieran Milan, et al. “Overcoming Catastrophic Forgetting in Neural Networks.” *Proceedings of the National Academy of Sciences* 114, no. 13 (2017): 3521–3526. https://doi.org/10.1073/pnas.1611835114

- Hooker, Sara, Nyalleng Moorosi, Gregory Clark, Samy Bengio, and Emily Denton. “Characterising Bias in Compressed Models.” *arXiv*, 2020. https://doi.org/10.48550/arXiv.2010.03058

## Lean Inference

- Habgood, John. “The Ethics of Resource Allocation: A Case Study.” *Journal of Medical Ethics* 9, no. 1 (March 1983): 21–24. https://doi.org/10.1136/jme.9.1.21

- Meta AI. “Llama 3.1 Model Card.” *Meta Llama Documentation*, accessed September 7, 2025. https://www.llama.com/docs/model-cards-and-prompt-formats/llama3_1/  See also: Meta AI, “Introducing Llama 3.1: Our Most Capable Models to Date,” July 23, 2024. https://ai.meta.com/blog/meta-llama-3-1/

- Avelar, Victor, Patrick Donovan, Paul Lin, Wendy Torell, and Maria A. Torres Arango. *The AI Disruption: Challenges and Guidance for Data Center Design*. Schneider Electric, White Paper 110, Version 1.1, September 7, 2023. https://media.datacenterdynamics.com/media/documents/Schneider_Electric_-_Modernization_WP110_V1.1_EN.pdf

- Iandola, Forrest N., Song Han, Matthew W. Moskewicz, Khalid Ashraf, William J. Dally, and Kurt Keutzer. “SqueezeNet: AlexNet-Level Accuracy with 50x Fewer Parameters and <0.5MB Model Size.” *arXiv*, 2016. https://doi.org/10.48550/arXiv.1602.07360

- Howard, Andrew G., Menglong Zhu, Bo Chen, Dmitry Kalenichenko, Weijun Wang, Tobias Weyand, Marco Andreetto, and Hartwig Adam. “MobileNets: Efficient Convolutional Neural Networks for Mobile Vision Applications.” *arXiv*, 2017. https://doi.org/10.48550/arXiv.1704.04861

- Tan, Mingxing, and Quoc V. Le. “EfficientNet: Rethinking Model Scaling for Convolutional Neural Networks.” *arXiv*, 2019. https://doi.org/10.48550/arXiv.1905.11946

- Wu, Bichen, Xiaoliang Dai, Peizhao Zhang, Yanghan Wang, Fei Sun, Yiming Wu, Yuandong Tian, Peter Vajda, Yangqing Jia, and Kurt Keutzer. “FBNet: Hardware-Aware Efficient ConvNet Design via Differentiable Neural Architecture Search.” In *Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)*, 2019. https://doi.org/10.1109/CVPR.2019.01099

- Cai, Han, Chuang Gan, Tianzhe Wang, Zhekai Zhang, and Song Han. “Once-for-All: Train One Network and Specialize It for Efficient Deployment.” *arXiv*, 2019. https://doi.org/10.48550/arXiv.1908.09791

- Bakhtiarifard, Pedram, Christian Igel, and Raghavendra Selvan. “EC-NAS: Energy Consumption Aware Tabular Benchmarks for Neural Architecture Search.” In *ICASSP 2024 – IEEE International Conference on Acoustics, Speech and Signal Processing*, 5660–5664. 2024. https://doi.org/10.1109/ICASSP48485.2024.10448303

- Miettinen, Kaisa. *Nonlinear Multiobjective Optimization*. New York: Springer, 1999. https://doi.org/10.1007/978-1-4615-5563-6

- Hinton, Geoffrey, Oriol Vinyals, and Jeff Dean. “Distilling the Knowledge in a Neural Network.” *arXiv*, 2015. https://doi.org/10.48550/arXiv.1503.02531

- Ren, Yuxin, Benyou Wang, Lifeng Shang, Xin Jiang, and Qun Liu. “Exploring Extreme Parameter Compression for Pre-trained Language Models.” *arXiv*, 2022. https://doi.org/10.48550/arXiv.2205.10036

- Kullback, Solomon, and Richard A. Leibler. “On Information and Sufficiency.” *The Annals of Mathematical Statistics* 22, no. 1 (March 1951): 79–86. https://doi.org/10.1214/aoms/1177729694

- Sanh, Victor, Lysandre Debut, Julien Chaumond, and Thomas Wolf. “DistilBERT, a Distilled Version of BERT: Smaller, Faster, Cheaper and Lighter.” *arXiv*, 2019. https://doi.org/10.48550/arXiv.1910.01108

- Li, Chaojian, Zhongzhi Yu, Yonggan Fu, Yongan Zhang, Yang Zhao, Haoran You, Qixuan Yu, Yue Wang, and Yingyan (Celine) Lin. “HW-NAS-Bench: Hardware-Aware Neural Architecture Search Benchmark.” *arXiv*, 2021. https://doi.org/10.48550/arXiv.2103.10584

- Frankle, Jonathan, and Michael Carbin. “The Lottery Ticket Hypothesis: Finding Sparse, Trainable Neural Networks.” *arXiv*, 2018. https://doi.org/10.48550/arXiv.1803.03635

- Oliver, B. M., J. R. Pierce, and C. E. Shannon. “The Philosophy of PCM.” *Proceedings of the IRE* 36, no. 11 (November 1948): 1324–1331. https://doi.org/10.1109/JRPROC.1948.231941

- Krishnamoorthi, Raghuraman, James Reed, Min Ni, Chris Gottbrath, and Seth Weidman. “Introduction to Quantization on PyTorch.” *PyTorch Blog*, March 26, 2020. https://pytorch.org/blog/introduction-to-quantization-on-pytorch/

- Pereira, Rui, Marco Couto, Francisco Ribeiro, Rui Rua, Jácome Cunha, João Paulo Fernandes, and João Saraiva. “Ranking Programming Languages by Energy Efficiency.” *Sustainable Computing: Informatics and Systems* 28 (2021): 100407. https://doi.org/10.1016/j.suscom.2021.100407

- Ahmed, Nur, and Muntasir Wahed. “The De-democratization of AI: Deep Learning and the Compute Divide in Artificial Intelligence Research.” *arXiv*, 2020. https://doi.org/10.48550/arXiv.2010.15581


## Hardware Considerations

- Crawford, Kate. *Atlas of AI: Power, Politics, and the Planetary Costs of Artificial Intelligence.* New Haven: Yale University Press, 2021.

- Ramesh, Aditya, Mikhail Pavlov, Gabriel Goh, Scott Gray, Chelsea Voss, Alec Radford, Mark Chen, and Ilya Sutskever. “Zero-Shot Text-to-Image Generation.” In *Proceedings of the 38th International Conference on Machine Learning (ICML 2021)*, PMLR 139: 8821–8831, 2021. https://doi.org/10.48550/arXiv.2102.12092

- McKinsey & Company. “AI Power: Expanding Data Center Capacity to Meet Growing Demand.” 2024. https://www.mckinsey.com/features/mckinsey-on-ai/our-insights/ai-power-expanding-data-center-capacity-to-meet-growing-demand

- Zuckerberg, Mark. “Announcing a New AI Data Center in Richland Parish, Louisiana.” Facebook, January 2025.

- International Energy Agency (IEA). *Electricity 2024: Analysis and Forecast to 2026.* Paris: IEA, 2024. https://iea.blob.core.windows.net/assets/70f034c0-528c-4177-8f4b-68c398c5b70b/Electricity2024.pdf

- Williams, Eric D., Robert U. Ayres, and Miriam Heller. “The 1.7 Kilogram Microchip: Energy and Material Use in the Production of Semiconductor Devices.” *Environmental Science & Technology* 36, no. 24 (2002): 5504–5510. https://doi.org/10.1021/es025643o

- Bux, Sebastian, Iris Heinrichs, Martin A. Böttcher, and Birgit Dröschel. “A Critical Analysis of Global Warming Potential of Data Centers in the Digital Era.” *Environment, Development and Sustainability* (2025). https://doi.org/10.1007/s10668-025-05434-z

- Baldé, Kees, Federico Magalini, Pascal Leroy, Dunzhu Li, Vanessa Forti, Cristina Giulia Di Giuseppe, Musa Sisay, Vishal Sharma, Deepali Sinha Khetriwal, and Ruediger Kuehr. *The Global E-waste Monitor 2024.* Bonn/Geneva/Rotterdam: UNITAR, ITU, and ISWA, 2024. https://ewastemonitor.info/wp-content/uploads/2024/03/GEM_2024-Final.pdf

- Wang, Peng, Ling-Yu Zhang, Asaf Tzachor, Eric Masanet, and Wei-Qiang Chen. “E-Waste Challenges of Generative Artificial Intelligence.” *Nature Computational Science* 4 (2024). https://doi.org/10.1038/s43588-024-00712-6

- Bashroush, Rabih. “A Comprehensive Reasoning Framework for Hardware Refresh in Data Centers.” *IT Professional* 20, no. 4 (2018): 33–41. https://doi.org/10.1109/MITP.2018.053631338

- Wright, Dustin, Christian Igel, Gabrielle Samuel, and Raghavendra Selvan. “Efficiency Is Not Enough: A Critical Perspective on Environmentally Sustainable AI.” *Communications of the ACM* 68, no. 7 (2025): 62–69. https://doi.org/10.1145/3724500

- Kaplan, Jared, Sam McCandlish, Tom Henighan, Tom B. Brown, Benjamin Chess, Rewon Child, Scott Gray, Alec Radford, Jeffrey Wu, and Dario Amodei. “Scaling Laws for Neural Language Models.” *arXiv*, January 23, 2020. https://doi.org/10.48550/arXiv.2001.08361

- Moore, Gordon E. “Cramming More Components onto Integrated Circuits.” *Electronics* 38, no. 8 (April 19, 1965): 114–117. https://www.cs.utexas.edu/~fussell/courses/cs352h/papers/moore.pdf

- Zhang, Nick. “Moore’s Law Is Dead, Long Live Moore’s Law!” *arXiv*, May 27, 2022. https://doi.org/10.48550/arXiv.2205.15011

- Koomey, Jonathan G., Stephen Berard, Marla Sanchez, and Henry Wong. “Implications of Historical Trends in the Electrical Efficiency of Computing.” *IEEE Annals of the History of Computing* 33, no. 3 (2011): 46–54. https://doi.org/10.1109/MAHC.2010.28

- Landauer, Rolf. “Irreversibility and Heat Generation in the Computing Process.” *IBM Journal of Research and Development* 5, no. 3 (1961): 183–191. https://doi.org/10.1147/rd.5.3.183

- Selvan, Raghavendra, Julian Schön, and Erik B. Dam. “Operating Critical Machine Learning Models in Resource Constrained Regimes.” In *Artificial Intelligence and Computer Vision*, Lecture Notes in Computer Science, 433–445. Cham: Springer, 2024. https://doi.org/10.1007/978-3-031-47425-5_29

- Landry, C., A. Jensen, A. O. Osherov, and A. L. McPartlin. “The Role of Artificial Intelligence in Radiotherapy Clinical Practice.” 2023.

- NVIDIA. “NVIDIA H100 Tensor Core GPU Data Sheet.” 2024. https://www.nvidia.com/en-us/data-center/h100/

- Pfister, Gregory. “Aspects of the InfiniBand Architecture.” In *Proceedings of the IEEE International Conference on Cluster Computing (CLUSTER 2001)*. 2001. https://www.computer.org/csdl/proceedings-article/cluster/2001/11160369/12OmNvDqsNA

- NVIDIA. “What Is NVLink?” 2023. https://www.nvidia.com/en-us/data-center/nvlink/what-is-nvlink/

- Goiri, Ismael, Kien Le, Thu D. Nguyen, Jordi Guitart, and Jordi Torres. “Greenslot: Scheduling Energy Consumption in Green Datacenters.” In *Proceedings of the International Conference on High Performance Computing, Networking, Storage and Analysis (SC ’11).* New York: ACM, 2011. https://doi.org/10.1145/2063384.2063411

- Radovanović, Ana, Ross Koningstein, Ian Schneider, Bokan Chen, Alexandre Duarte, Binz Roy, Diyue Xiao, Maya Haridasan, Patrick Hung, Nick Care, Saurav Talukdar, Eric Mullen, Kendal Smith, MariEllen Cottman, and Walfredo Cirne. “Carbon-Aware Computing for Datacenters.” *arXiv*, June 11, 2021. https://doi.org/10.48550/arXiv.2106.11750

- Ahmed, Nur, and Muntasir Wahed. “The De-democratization of AI: Deep Learning and the Compute Divide in Artificial Intelligence Research.” *arXiv*, October 22, 2020. https://doi.org/10.48550/arXiv.2010.15581

- Raina, Rajat, Anand Madhavan, and Andrew Y. Ng. “Large-Scale Deep Unsupervised Learning Using Graphics Processors.” In *Proceedings of the 26th International Conference on Machine Learning (ICML ’09)*, 2009.

- Cusumano, Michael A. “NVIDIA at the Center of the Generative AI Ecosystem—For Now.” *Communications of the ACM* 67, no. 1 (2024): 33–35. https://doi.org/10.1145/3631537

- Jeon, Myeongjae, Shivaram Venkataraman, Amar Phanishayee, Junjie Qian, Wencong Xiao, and Fan Yang. “Analysis of Large-Scale Multi-Tenant GPU Clusters for DNN Training Workloads.” In *2019 USENIX Annual Technical Conference (USENIX ATC 19)*, 947–960. 2019. https://www.usenix.org/conference/atc19/presentation/jeon

- Robroek, Ties T. T. *Resourceful Learning: Training More Models with Fewer Resources.* PhD thesis, IT University of Copenhagen, 2024. https://en.itu.dk/-/media/EN/Research/PhD-Programme/PhD-defences/2024/PhD-thesis-Temporary-Version-TR-pdf.pdf

- Robroek, Ties, Ehsan Yousefzadeh-Asl-Miandoab, and Pınar Tözün. “An Analysis of Collocation on GPUs for Deep Learning Training.” In *EuroSys ’24 Companion: Proceedings of the 4th Workshop on Machine Learning and Systems (EuroMLSys 2024).* New York: ACM, 2024. https://doi.org/10.1145/3642970.3655827

- Tang, Zhenheng, Yuxin Wang, Qiang Wang, and Xiaowen Chu. “The Impact of GPU DVFS on the Energy and Performance of Deep Learning: An Empirical Study.” In *Proceedings of the 10th ACM International Conference on Future Energy Systems (e-Energy ’19)*, 315–325. 2019. https://doi.org/10.1145/3307772.3328315

- Tang, Zhuo, Ling Qi, Zhenzhen Cheng, Kenli Li, Samee U. Khan, and Keqin Li. “An Energy-Efficient Task Scheduling Algorithm in DVFS-Enabled Cloud Environment.” *Journal of Grid Computing* 14, no. 1 (2016): 55–74. https://doi.org/10.1007/s10723-015-9334-y

- Wu, Chun-Ming, Ruay-Shiung Chang, and Hsin-Ying Chan. “A Green Energy-Efficient Scheduling Algorithm Using the DVFS Technique for Cloud Datacenters.” *Future Generation Computer Systems* 37 (2014): 141–147. https://www.sciencedirect.com/science/article/abs/pii/S0167739X13001234

- Arafa, Ahmed, Zinan Lin, Hanyang Mao, Abu Sebastian, and Onur Mutlu. “Verified Instruction-Level Energy Consumption Measurement for NVIDIA GPUs.” 2020.

- Delestrac, Maxime, et al. “Analyzing GPU Energy Consumption in Data Movement and Storage.” 2024.

- Li, Pengfei, Jianyi Yang, Mohammad A. Islam, and Shaolei Ren. “Making AI Less ‘Thirsty’: Uncovering and Addressing the Secret Water Footprint of AI Models.” *Communications of the ACM* 68, no. 7 (2025): 54–61. https://doi.org/10.1145/3724499

- Kaack, Lynn H., Vinodkumar Prabhakaran, Peter Henderson, Timnit Gebru, Kate Crawford, Priya Donti, David Rolnick, and Emma Strubell. “Aligning Artificial Intelligence with Climate Change Mitigation.” *Nature Climate Change* 12 (2022): 518–527. https://doi.org/10.1038/s41558-022-01377-7

## A Recipe For Sustainable AI

- Jones, Capers. *The Year 2000 Software Problem: Quantifying the Costs and Assessing the Consequences*. Addison-Wesley, 1997.

- Cunningham, Ward. “The WyCash Portfolio Management System.” In *Addendum to the Proceedings of the ACM Conference on Object-Oriented Programming Systems, Languages, and Applications (OOPSLA ’92)*, 29–30. 1992. https://doi.org/10.1145/157709.157715

- Sculley, D., Gary Holt, Daniel Golovin, Eugene Davydov, Todd Phillips, Dietmar Ebner, Vinay Chaudhary, Michael Young, Jean-François Crespo, and Dan Dennison. “Hidden Technical Debt in Machine Learning Systems.” In *Advances in Neural Information Processing Systems 28 (NeurIPS 2015)*, 2503–11. 2015. https://papers.nips.cc/paper/5656-hidden-technical-debt-in-machine-learning-systems.pdf

- Kapoor, Sayash, and Arvind Narayanan. “Leakage and the Reproducibility Crisis in ML-Based Science.” *Patterns* 4, no. 9 (2023): 100804. https://doi.org/10.1016/j.patter.2023.100804

- Kaplan, Jared, Sam McCandlish, Tom Henighan, Tom B. Brown, Benjamin Chess, Rewon Child, Scott Gray, Alec Radford, Jeffrey Wu, and Dario Amodei. “Scaling Laws for Neural Language Models.” *arXiv*, January 23, 2020. https://doi.org/10.48550/arXiv.2001.08361

- Heidorn, P. Bryan. “Shedding Light on the Dark Data in the Long Tail of Science.” *Library Trends* 57, no. 2 (2008): 280–299. https://doi.org/10.1353/lib.0.0036

- Evchenko, Mikhail, Joaquin Vanschoren, Holger H. Hoos, Marc Schoenauer, and Michèle Sebag. “Frugal Machine Learning.” *arXiv*, November 5, 2021. https://doi.org/10.48550/arXiv.2111.03731

- DeepSeek-AI, et al. “DeepSeek-V3 Technical Report.” *arXiv*, December 27, 2024 (rev. February 18, 2025). https://doi.org/10.48550/arXiv.2412.19437

- Spoczynski, Marcin, Marcela S. Melara, and Sebastian Szyller. “Atlas: A Framework for ML Lifecycle Provenance & Transparency.” *arXiv*, February 26, 2025. https://doi.org/10.48550/arXiv.2502.19567

- Selvan, Raghavendra, Kazem Mostafavi Isfahani, Martin Eklund, and Rasmus Reinhold Paulsen. “PePR: Performance Per Resource Unit as a Metric to Promote Small-Scale Deep Learning in Medical Image Analysis.” *Proceedings of Machine Learning Research* 265 (2025). https://proceedings.mlr.press/v265/selvan25a.html

- International Organization for Standardization/International Electrotechnical Commission/IEEE. *ISO/IEC/IEEE 32675:2022 Information Technology—DevOps—Building Reliable and Secure Systems Including Application Build, Package and Deployment.* Geneva: ISO/IEC/IEEE, 2022. https://www.iso.org/standard/83132.html

- Salama, Khalid, Jarek Kazmierczak, and Donna Schut. *Practitioners Guide to MLOps: A Framework for Continuous Delivery and Automation of Machine Learning.* Google Cloud, May 2021. https://services.google.com/fh/files/misc/practitioners_guide_to_mlops_whitepaper.pdf

- MLOps SIG. “MLOps Principles.” 2025. https://ml-ops.org/content/mlops-principles

- Mitchell, Margaret, Simone Wu, Andrew Zaldivar, Parker Barnes, Lucy Vasserman, Ben Hutchinson, Elena Spitzer, Inioluwa Deborah Raji, and Timnit Gebru. “Model Cards for Model Reporting.” In *Proceedings of the Conference on Fairness, Accountability, and Transparency (FAT\*)*, 220–229. New York: ACM, 2019. https://doi.org/10.1145/3287560.3287596

- Inie, Nanna, Jeanette Falk, and Raghavendra Selvan. “How CO2STLY Is CHI? The Carbon Footprint of Generative AI in HCI Research and What We Should Do About It.” In *Proceedings of the 2025 CHI Conference on Human Factors in Computing Systems (CHI ’25).* New York: ACM, 2025. https://doi.org/10.1145/3706598.3714227

- McMahon, James E., and Stephen Wiel. *Energy-Efficiency Labels and Standards: A Guidebook for Appliances, Equipment and Lighting.* Washington, DC: Collaborative Labeling and Appliance Standards Program (CLASP), 2001. https://doi.org/10.2172/836221

- Fischer, Raphael, Matthias Jakobs, and Katharina Morik. “Energy Efficiency Considerations for Popular AI Benchmarks.” *arXiv*, April 17, 2023. https://doi.org/10.48550/arXiv.2304.08359

## Toward Sustainable AI

